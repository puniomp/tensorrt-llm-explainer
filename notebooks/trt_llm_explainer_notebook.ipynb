{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612e5677",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding TensorRT-LLM: Kernel Fusion, Memory Efficiency, and High-Throughput LLM Inference\n",
    "\n",
    "**Author:** Marco Punio  \n",
    "\n",
    "This notebook is a conceptual and light-code explainer of why **NVIDIA TensorRT-LLM (TRT-LLM)** can deliver **2‚Äì4√ó faster LLM inference** compared to framework baselines like vanilla PyTorch, while also being more efficient than many general-purpose runtimes.\n",
    "\n",
    "> üîé Note: This notebook is **CPU-only** and does **not** require a GPU or TensorRT-LLM installation. The goal is to understand **architecture and performance reasoning**, not to run real TRT-LLM code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636b178",
   "metadata": {},
   "source": [
    "\n",
    "## 1. The LLM Inference Bottleneck: Prefill vs Decode\n",
    "\n",
    "LLM inference is usually split into two phases:\n",
    "\n",
    "- **Prefill (context ingestion)**: process the full input sequence once.\n",
    "- **Decode (token generation)**: generate tokens one-by-one while reusing a **KV cache**.\n",
    "\n",
    "### 1.1 Prefill Phase\n",
    "\n",
    "- Long sequence length (e.g., 1K‚Äì8K tokens).\n",
    "- Large matrix multiplications (GEMMs) dominate.\n",
    "- GPU can reach high Tensor Core utilization.\n",
    "- Often **compute-bound**.\n",
    "\n",
    "### 1.2 Decode Phase\n",
    "\n",
    "- We generate **one token per step per sequence**.\n",
    "- Each new token has to attend over all previous tokens using the KV cache.\n",
    "- Much smaller effective batch size at each step.\n",
    "- Often **memory-bound** because we keep re-reading the KV cache.\n",
    "\n",
    "This means **prefill and decode have different bottlenecks**, which matters when thinking about why TRT-LLM is faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88db6973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq len =  256, simulated attention latency = 0.0026 s\n",
      "Seq len =  512, simulated attention latency = 0.0043 s\n",
      "Seq len = 1024, simulated attention latency = 0.0056 s\n",
      "Seq len = 2048, simulated attention latency = 0.0333 s\n",
      "Seq len = 4096, simulated attention latency = 0.0165 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def naive_attention_step(seq_len=2048, d_model=128):\n",
    "    \"\"\"CPU-only simulation of a single attention step.\n",
    "    This is NOT optimized and is just for timing intuition.\"\"\"\n",
    "    Q = np.random.randn(1, d_model)\n",
    "    K = np.random.randn(seq_len, d_model)\n",
    "    V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "    scores = Q @ K.T                     # (1, seq_len)\n",
    "    probs = np.exp(scores) / np.exp(scores).sum()\n",
    "    out = probs @ V                      # (1, d_model)\n",
    "    return out\n",
    "\n",
    "for L in [256, 512, 1024, 2048, 4096]:\n",
    "    t0 = time.time()\n",
    "    _ = naive_attention_step(seq_len=L)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Seq len = {L:4d}, simulated attention latency = {dt:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122cb63",
   "metadata": {},
   "source": [
    "### Key takeaways running first cell\n",
    "\n",
    "- This matches the theoretical O(N) scaling of full attention latency growing linearly with sequence length because of QK^T\n",
    "- The first timing measurement is not reliable due to library warmup\n",
    "- BLAS kernel selection explains why 512 tokens can be faster than 256 tokens\n",
    "- This expirement reveals meaningful transformer intuition even without a GPU\n",
    "- The scaling trend shown here is the same one optimized by NVIDIA cuBLAS and TensortRT-LLM\n",
    "\n",
    "### Using NumPy to understand algorthmic intuition\n",
    "\n",
    "The NumPy example is intentionally CPU-only. Its purpose is to illustrate ***what attention computes*** and why full attention scales as **O(sequence_length)**.\n",
    "NumPy is perfect for this because it exposes the math directly:\n",
    "- Q @ K·µÄ\n",
    "- softmax\n",
    "- attention √ó V\n",
    "\n",
    "This demo helps us understand *why* attention becomes expensive as context grows.\n",
    "\n",
    "However, NumPy does **not** reflect how real frameworks execute attention on GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd95e9b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Why Framework Baselines Struggle (Vanilla PyTorch/Eager)\n",
    "\n",
    "A typical transformer layer in an LLM involves a bunch of the following operations:\n",
    "\n",
    "- Linear projections for Q, K, V\n",
    "- Attention (softmax over QK·µÄ)\n",
    "- Feed-forward network (MLP)\n",
    "- Residual connections\n",
    "- LayerNorm\n",
    "\n",
    "In a naive framework execution (e.g., vanilla PyTorch eager mode):\n",
    "\n",
    "- Each operation often launches a **separate kernel** on the GPU.\n",
    "- Each kernel reads/writes from global memory.\n",
    "- The CPU runtime / dispatcher adds overhead between kernel launches.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "- Many small kernels per token during **decode**.\n",
    "- High **kernel launch overhead**.\n",
    "- High **memory traffic** (read/write between each op).\n",
    "- Inability to fully saturate Tensor Cores, especially in decode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f714d",
   "metadata": {},
   "source": [
    "\n",
    "## 3. What TensorRT-LLM Does Differently (High Level)\n",
    "\n",
    "**TensorRT-LLM** is a specialized runtime and compiler stack for transformer inference that focuses on:\n",
    "\n",
    "1. **Graph-level optimization**  \n",
    "   - Capture the LLM graph.\n",
    "   - Fuse compatible ops into bigger kernels.\n",
    "   - Reorder operations for better memory access patterns.\n",
    "\n",
    "2. **Fused attention and MLP kernels**  \n",
    "   - Combine multiple operations into a single kernel launch.\n",
    "   - Greatly reduce memory round-trips.\n",
    "\n",
    "3. **Efficient KV cache management (paged KV cache)**  \n",
    "   - Avoid large, contiguous allocations that fragment memory.\n",
    "   - Use block-based layouts that scale better with sequence length and batch size.\n",
    "\n",
    "4. **Dynamic batching and scheduling**  \n",
    "   - Batch tokens across multiple sequences at runtime.\n",
    "   - Maximize throughput without blowing up latency.\n",
    "\n",
    "5. **Lower precision (FP8/INT8) support**  \n",
    "   - Use Tensor Cores more effectively.\n",
    "   - Reduce memory bandwidth and storage per parameter.\n",
    "\n",
    "\n",
    "TensorRT-LLM is similar in spirit to TorchDynamo and TorchInductor in that it\n",
    "captures the model graph, fuses operations, and generates optimized kernels.\n",
    "However, TRT-LLM goes much further: it also provides a full inference runtime,\n",
    "paged KV cache management, dynamic batching, FP8 Tensor Core kernels, and\n",
    "serving-level optimizations. It is not just a compiler ‚Äî it is a complete\n",
    "LLM inference engine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8815c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. KV Cache and Paged Attention (Conceptual)\n",
    "\n",
    "During decode, we reuse the previously computed key/value tensors (K, V) so that generating token *t* does **not** require recomputing attention against all tokens *< t*. These cached tensors form the **KV cache**.\n",
    "\n",
    "### 4.1 Naive KV Cache Layout\n",
    "\n",
    "In a naive implementation:\n",
    "\n",
    "- Each sequence allocates one large contiguous KV buffer\n",
    "- As sequence grow at different rates, memory becomes fragmented.\n",
    "- If the buffer needs to grow, frameworks may need to 'realloc' or copy large amounts of memory.\n",
    "- This becomes expensive when serving many concurrent sequences.\n",
    "\n",
    "This layout is simple but does not scale well for real-time multi-user serving workloads.\n",
    "\n",
    "### 4.2 Paged KV Cache (High Level)\n",
    "\n",
    "Paged KV cache avoids these problems by splitting the KV memory into **fixed-size blocks (\"pages\")**, typically 16-128 tokens each.\n",
    "\n",
    "A small **indirection table** maps logical sequence positions -> physical memory blocks.\n",
    "\n",
    "This gives several benefits:\n",
    "\n",
    "- **No large reallocations:** Growing a sequence only means attaching another page, not resizing a giant buffer.\n",
    "- **Memory reuse:** Freed pages can be recycled for other sequences.\n",
    "- **Reduced fragmentation:** The system only manages uniform blocks.\n",
    "- **Efficient batching:** Pages align sequences so that multiple sequences can be processed together. \n",
    "\n",
    "Paged KV cache is one of the key enablers behind high-throughput serving systems like vLLM and TensorRT-LLM.\n",
    "\n",
    "### Important Clarification: Paged Attention Does *Not* Reduce Compute\n",
    "\n",
    "Even with paging, attention still computes against **all previous tokens**.\n",
    "Splitting K/V into blocks changes **how we access memory**, but it does *not*\n",
    "change the algorithmic cost.\n",
    "\n",
    "This is why the simulated paged-attention NumPy cell has latency numbers nearly\n",
    "identical to the full-attention version‚Äîthe computation is the same, only the\n",
    "memory layout is different.\n",
    "\n",
    "On real GPUs, however, the paged layout:\n",
    "\n",
    "- Improves memory locality,\n",
    "- Enables kernel fusion,\n",
    "- Reduces overhead under high concurrency,\n",
    "- Eliminates fragmentation,\n",
    "\n",
    "all of which significantly improve **system-level performance**, even though the\n",
    "core math is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2062ba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq len =  256, simulated paged attention = 0.0152 s\n",
      "Seq len =  512, simulated paged attention = 0.0020 s\n",
      "Seq len = 1024, simulated paged attention = 0.0088 s\n",
      "Seq len = 2048, simulated paged attention = 0.0077 s\n",
      "Seq len = 4096, simulated paged attention = 0.0163 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "BLOCK = 64  # tokens per \"page\"\n",
    "\n",
    "def paged_attention_step(seq_len=2048, d_model=128, block_size=BLOCK):\n",
    "    \"\"\"CPU-only simulation of a block-wise attention pattern.\n",
    "    Still not optimized, but mirrors the idea of processing in chunks.\"\"\"\n",
    "    Q = np.random.randn(1, d_model)\n",
    "    num_blocks = math.ceil(seq_len / block_size)\n",
    "    out = np.zeros((1, d_model))\n",
    "\n",
    "    for b in range(num_blocks):\n",
    "        this_block_size = min(block_size, seq_len - b * block_size)\n",
    "        K_block = np.random.randn(this_block_size, d_model)\n",
    "        V_block = np.random.randn(this_block_size, d_model)\n",
    "        scores = Q @ K_block.T\n",
    "        probs = np.exp(scores) / np.exp(scores).sum()\n",
    "        out += probs @ V_block\n",
    "\n",
    "    return out\n",
    "\n",
    "for L in [256, 512, 1024, 2048, 4096]:\n",
    "    t0 = time.time()\n",
    "    _ = paged_attention_step(seq_len=L)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Seq len = {L:4d}, simulated paged attention = {dt:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be34e58",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Kernel Fusion: Fewer, Heavier Kernels\n",
    "\n",
    "A simplified unfused pipeline for a feed-forward block might look like:\n",
    "\n",
    "1. Linear layer (GEMM)\n",
    "2. Add bias\n",
    "3. Apply activation (e.g., GELU)\n",
    "4. Another linear layer\n",
    "5. Add residual connection\n",
    "\n",
    "Each of these steps might be a separate kernel launch in a naive implementation.\n",
    "\n",
    "### 5.1 Unfused vs Fused (Conceptual)\n",
    "\n",
    "- **Unfused**:  \n",
    "  - Each step reads from and writes to global memory.  \n",
    "  - Many small kernels, each doing relatively little work.\n",
    "\n",
    "- **Fused**:  \n",
    "  - Combine multiple steps into one kernel.  \n",
    "  - Data stays in registers or shared memory longer.  \n",
    "  - Much fewer global memory accesses.\n",
    "\n",
    "On GPUs, **global memory bandwidth** is a major bottleneck. Fusing kernels reduces trips to global memory, which directly boosts throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b63218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between unfused and fused: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def unfused_mlp(x, W1, b1, W2, b2):\n",
    "    # Many conceptual \"kernels\"\n",
    "    h = x @ W1       # GEMM\n",
    "    h = h + b1       # add bias\n",
    "    h = np.tanh(h)   # activation\n",
    "    out = h @ W2     # GEMM\n",
    "    out = out + b2   # add bias\n",
    "    return out\n",
    "\n",
    "def fused_mlp(x, W1, b1, W2, b2):\n",
    "    # Same math, but conceptually in one kernel\n",
    "    h = x @ W1\n",
    "    h = np.tanh(h + b1)\n",
    "    out = h @ W2 + b2\n",
    "    return out\n",
    "\n",
    "# Just to show they're numerically close\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(2, 4)\n",
    "W1 = np.random.randn(4, 8)\n",
    "b1 = np.random.randn(8)\n",
    "W2 = np.random.randn(8, 4)\n",
    "b2 = np.random.randn(4)\n",
    "\n",
    "u = unfused_mlp(x, W1, b1, W2, b2)\n",
    "f = fused_mlp(x, W1, b1, W2, b2)\n",
    "print(\"Max difference between unfused and fused:\", np.max(np.abs(u - f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb292e",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "The unfused and fused versions produce the exact same output because fusion does *not* change the math. In this NumPy CPU example, they also appear the same speed because NumPy does not launch GPU kernels - everything runs as plain CPU ops.\n",
    "\n",
    "The goal of this example is just to show that fusion does not affect correctness. The *performance* benefits of fusion only show up on GPUs, where unfused execution would launch many small kernels and bounce data through global memory. TensorRT-LLM fuses these ops into a single optimized GPU kernel, which is why fused MLPs are dramatically faster in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9bd4d",
   "metadata": {},
   "source": [
    "\n",
    "## 6. TRT-LLM vs PyTorch vs vLLM (Conceptual Comparison)\n",
    "\n",
    "We can compare three broad approaches to LLM inference:\n",
    "\n",
    "| Feature                         | Vanilla PyTorch | vLLM                         | TensorRT-LLM                     |\n",
    "|---------------------------------|-----------------|-----------------------------|----------------------------------|\n",
    "| Execution mode                  | Eager (dynamic) | Runtime optimized for LLMs  | Ahead-of-time + runtime optimized |\n",
    "| Kernel fusion                   | Limited         | Some fusion & optimizations | Aggressive fusion via TensorRT   |\n",
    "| KV cache layout                 | Basic/naive     | PagedAttention              | Paged KV + TensorRT-optimized   |\n",
    "| Dynamic batching                | Manual/custom   | Built-in                    | Built-in                         |\n",
    "| Precision support (FP8/INT8)    | Limited         | Limited / external          | First-class, Tensor Core aware   |\n",
    "| Target use-case                 | Research/dev    | High-throughput serving     | High-throughput, low-latency prod |\n",
    "\n",
    "The key takeaway is that **TRT-LLM** is built to **minimize memory traffic and kernel overhead** while maximizing **Tensor Core utilization**, especially during **decode**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb0102",
   "metadata": {},
   "source": [
    "\n",
    "## 7. What Happens When We Generate a Single Token? (Conceptual Walkthrough)\n",
    "\n",
    "For a single decode step on a batch of sequences, a very simplified view:\n",
    "\n",
    "### 7.1 Vanilla PyTorch\n",
    "\n",
    "1. Launch kernel for Q, K, V projections.\n",
    "2. Launch kernel for QK·µÄ.\n",
    "3. Launch kernel for scaling + masking.\n",
    "4. Launch kernel for softmax.\n",
    "5. Launch kernel for attention * V.\n",
    "6. Launch kernels for MLP and residuals.\n",
    "7. Launch kernel for LayerNorm.\n",
    "\n",
    "‚û°Ô∏è Many small kernels, many reads/writes to global memory.\n",
    "\n",
    "### 7.2 vLLM\n",
    "\n",
    "- Uses **paged attention** and runtime tactics to:\n",
    "  - Reuse KV cache efficiently.\n",
    "  - Batch multiple sequences.\n",
    "  - Reduce some overhead compared to pure eager mode.\n",
    "\n",
    "Still, many operations remain distinct kernels.\n",
    "\n",
    "### 7.3 TensorRT-LLM\n",
    "\n",
    "- Ahead-of-time optimized graph.\n",
    "- **Fused attention kernels** that combine several of the above steps.\n",
    "- Fused MLPs.\n",
    "- Layouts chosen to best align with Tensor Cores.\n",
    "- Lower precision (e.g., FP8) paths where appropriate.\n",
    "\n",
    "‚û°Ô∏è **Far fewer kernels per token**, and each kernel does more useful work per byte of memory accessed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7157d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Performance Reasoning (Back-of-the-Envelope)\n",
    "\n",
    "We can think of tokens/sec as being limited by:\n",
    "\n",
    "- **Compute throughput** (FLOPs / second).\n",
    "- **Memory bandwidth** (bytes / second).\n",
    "- **Kernel launch overhead** (CPU ‚Üî GPU coordination).\n",
    "\n",
    "During **decode**, memory bandwidth and launch overhead often dominate.\n",
    "\n",
    "### 8.1 Very Rough Toy Model\n",
    "\n",
    "Let:\n",
    "\n",
    "- `T_py` = time per token in vanilla PyTorch.\n",
    "- `T_trt` = time per token in TRT-LLM.\n",
    "\n",
    "If TRT-LLM:\n",
    "\n",
    "- Reduces global memory reads/writes by ~2√ó through fusion.\n",
    "- Reduces kernel launches per token from ~N to ~N/2 or less.\n",
    "- Uses lower precision to double effective bandwidth.\n",
    "\n",
    "Then it's entirely plausible for:\n",
    "\n",
    "\\[ T_{trt} \\approx \\frac{1}{2} T_{py} \\quad \\text{to} \\quad \\frac{1}{4} T_{py} \\]\n",
    "\n",
    "which matches the **2‚Äì4√ó throughput improvement** often reported in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dbfa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch cost per 1M tokens:  $1.67\n",
      "TRT-LLM  cost per 1M tokens: $0.56\n",
      "Cost reduction factor: 3.00x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def estimate_cost_per_million_tokens(tokens_per_second, gpu_hourly_cost):\n",
    "    \"\"\"Very rough cost-per-million-tokens model.\"\"\"\n",
    "    seconds_per_million = 1_000_000 / tokens_per_second\n",
    "    hours = seconds_per_million / 3600\n",
    "    return hours * gpu_hourly_cost\n",
    "\n",
    "tps_pytorch = 500   # toy numbers\n",
    "tps_trtllm  = 1500  # 3x faster\n",
    "\n",
    "gpu_cost = 3.0  # $/GPU-hour, example\n",
    "\n",
    "cost_pytorch = estimate_cost_per_million_tokens(tps_pytorch, gpu_cost)\n",
    "cost_trtllm  = estimate_cost_per_million_tokens(tps_trtllm, gpu_cost)\n",
    "\n",
    "print(f\"PyTorch cost per 1M tokens:  ${cost_pytorch:.2f}\")\n",
    "print(f\"TRT-LLM  cost per 1M tokens: ${cost_trtllm:.2f}\")\n",
    "print(f\"Cost reduction factor: {cost_pytorch / cost_trtllm:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fe8b1",
   "metadata": {},
   "source": [
    "### What this cost model shows\n",
    "\n",
    "This toy example illustrates a simple but important point: if a model produces more tokens per second, it directly reduces the cost of serving tokens. The cost per million tokens is proportional to the number of GPU-hours needed. If TensorRT-LLM delivers 3x higher throughput than PyTorch (1500 vs 500 tokens/sec in this example), then the GPU spends 3x less time generating the same number of tokens. Since GPU pricing is based on time, the cost per million tokens drop by roughly the same factor.\n",
    "\n",
    "This is why throughput optimizations translates directly into cost savings in production LLM workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dbe9ca",
   "metadata": {},
   "source": [
    "\n",
    "## 9. When to Use TensorRT-LLM vs vLLM (Customer Guidance)\n",
    "\n",
    "### TensorRT-LLM is a great fit when:\n",
    "\n",
    "- You need **maximum throughput** and **low latency** in production.\n",
    "- You're serving **large models** (e.g., 13B, 34B, 70B+).\n",
    "- You care about **cost-per-token** at scale.\n",
    "- You can invest engineering time to integrate a more specialized runtime.\n",
    "- You want to exploit **FP8/INT8** and advanced kernel fusion.\n",
    "\n",
    "### vLLM is a great fit when:\n",
    "\n",
    "- You want a simpler runtime with strong performance.\n",
    "- You prioritize ease-of-use and fast experimentation.\n",
    "- You may still be iterating on models and prompts frequently.\n",
    "- You don't need the absolute maximum throughput that TRT-LLM can offer.\n",
    "\n",
    "As a Solutions Architect, the key is to:\n",
    "\n",
    "- Understand the **customer's constraints** (latency, throughput, budget, model size).\n",
    "- Understand the **technical bottlenecks** (compute vs memory vs networking).\n",
    "- Recommend the stack that aligns with both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf32b2c",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Summary and Talking Points\n",
    "\n",
    "By now, you should be able to explain:\n",
    "\n",
    "- Why LLM **decode** tends to be **memory-bound**.\n",
    "- How **kernel fusion** in TensorRT-LLM reduces memory traffic and kernel launch overhead.\n",
    "- What **paged KV cache** is and why it helps under high concurrency.\n",
    "- How **FP8/INT8** and Tensor Cores contribute to higher throughput.\n",
    "- Conceptual differences between **PyTorch**, **vLLM**, and **TensorRT-LLM**.\n",
    "\n",
    "### Handy interview / customer-facing bullets\n",
    "\n",
    "- *\"TRT-LLM reduces the number of kernels per token and the amount of global memory traffic, which is why it often delivers 2‚Äì4√ó higher decode throughput than vanilla PyTorch.\"*\n",
    "- *\"Paged KV cache and dynamic batching are crucial for high-concurrency LLM serving because they keep memory usage predictable and avoid fragmentation.\"*\n",
    "- *\"The real value to customers isn't just speed in isolation, it's **cost per token** and **latency under load**, which is where TRT-LLM really shines.\"*\n",
    "\n",
    "You can now expand this notebook by:\n",
    "\n",
    "- Adding simple benchmark experiments on CPU (for educational purposes).\n",
    "- Sketching diagrams of execution graphs.\n",
    "- Linking to real TRT-LLM docs or repos in Markdown cells.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRT Env",
   "language": "python",
   "name": "trt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
